{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fda72a13-e5e6-4224-9885-e3e99d95e73f",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/UNSW-COMP9414/Tutorials/blob/main/Week08/COMP9414-Week08-Tree-Models-Solution.ipynb)\n",
    "\n",
    "# Training and Assessing Tree-based Models with Scikit-Learn\n",
    "\n",
    "**COMP9414 W08 Tutorial**\n",
    "\n",
    "- Instructor: Gustavo Batista\n",
    "- School of Computer Science and Engineering, UNSW Sydney\n",
    "- Notebook designed by Gustavo Batista\n",
    "- Last Update 24th October 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253e470a-52a9-4fca-b17a-fedd35efbcc5",
   "metadata": {},
   "source": [
    "In this week's tutorial, we will train and assess three tree-based models: decision trees, bagging of trees and random forests. We will start with the Iris dataset to gain intuition about cutpoints and splitting criteria. Later, we will use the Wine Quality dataset to train, evaluate and compare our tree-based model.\n",
    "\n",
    "We will use the Scikit-Learn library, which provides an extensive collection of Machine Learning classifiers and regressors, among other useful functions for data preparation and model assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ae70bb-242e-4a62-b0de-c45c05743955",
   "metadata": {},
   "source": [
    "## Technical prerequisites\n",
    "\n",
    "You will need the following packages installed to run this notebook:\n",
    "\n",
    "1. Numpy\n",
    "2. Pandas\n",
    "3. Matplotlib\n",
    "4. Seaborn\n",
    "5. Scikit-learn\n",
    "\n",
    "These libraries are often found in most installations. If they are not installed on your system, you can install them using the `pip` or `conda` commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f75e0c0-45f2-4aab-9ba0-fa2724c38d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy and matplotlib libraries for numerical computation and plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn libraries for dataset\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Scikit-learn libraries for models\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "# Scikit-learn libraries for model assessment and hyperparameter search\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f4b9d3-d231-4917-ae52-1a0347966928",
   "metadata": {},
   "source": [
    "## The Iris dataset\n",
    "\n",
    "We will start with a simple decision tree classifier using the Iris dataset. \n",
    "\n",
    "The Iris dataset is a classic benchmark dataset. It contains 150 samples of iris flowers, divided equally among three species: setosa, versicolor, and virginica. Each sample has four numerical features: sepal length, sepal width, petal length, and petal width, all measured in centimetres. The target variable indicates the flower's species, encoded as 0 (Setosa), 1 (Versicolor), and 2 (Virginica).\n",
    "\n",
    "Like MNIST, this dataset is very popular for benchmarking Machine Learning models. It is even available as part of the Scikit-learn installation.\n",
    "\n",
    "We can use the following command to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f844e41-77e3-473d-b7b8-a614c4d04b84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Convert it into a Pandas DataFrame\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_df['target'] = iris.target\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(iris_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30efb0a-322a-47bc-b94a-d3ec59a14035",
   "metadata": {},
   "source": [
    "## Exploratory data analysis\n",
    "\n",
    "Before building our classifiers, it is good to get a better understanding of the data. A extremelly popular plot is known as the scatterplot matrix (among other names). This is matrix of scatterplots comparing pairs of attributes.\n",
    "\n",
    "We can easily create a scatterplot matrix using the `pairplot` function of the Seaboarn library. The next cell creates this plot for the Iris dataset. Before creating the plot, we will rename the classes, from 0, 1 and 2 to the actual class labels: setosa, versicolor and virginica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c7c3db-97fd-43a9-bfca-697f83ee2778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map target integers to class names for better visualization\n",
    "iris_df['species'] = iris_df['target'].map({0: 'Setosa', 1: 'Versicolor', 2: 'Virginica'})\n",
    "\n",
    "# Create a scatter plot matrix\n",
    "sns.pairplot(iris_df, hue='species', diag_kind='kde', palette='bright', markers=[\"o\", \"s\", \"D\"])\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0822cacb-79a2-4af0-b309-01203a35292d",
   "metadata": {},
   "source": [
    "The scatterplot matrix shows that the class setosa can be easily separated from the other two. For instance, simple cuts such as petal length around two and petal width around one will create a terminal node for all setosa cases.\n",
    "\n",
    "In the next section, we will use this information to assess the node splitting measures Gini index and cross-entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe6aaa3-3252-4a98-8ebd-6d143dfe1a92",
   "metadata": {},
   "source": [
    "## Calculation Gini and Cross-entropy\n",
    "\n",
    "Let's start with an exercise to compute the Gini index and cross-entropy. We can calculate the Gini index with the following equation:\n",
    "\n",
    "$$G = \\sum_{k=1}^{K} \\hat{p}_{mk}(1-\\hat{p}_{mk}) $$\n",
    "\n",
    "While the cross-entropy is defined as:\n",
    "\n",
    "$$D = - \\sum_{k=1}^{K} \\hat{p}_{mk} \\log_2 \\hat{p}_{mk} $$\n",
    "\n",
    "Where $K$ is the number of classes and $\\hat{p}_{mk}$ is the class distribution for the class $k$ in a region $m$.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Implement the functions ``compute_gini(p)`` and ``compute_cross_entropy(p)`` that compute the Gini index and cross-entropy for a vector of class distributions ``p``. The vector ``p`` has $K$ values in the $[0, 1]$ range that sum up to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e93d640-ffee-488f-95c1-48031266ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cross_entropy(p):\n",
    "    \"\"\"\n",
    "    Compute the cross-entropy of a probability distribution.\n",
    "\n",
    "    Parameters:\n",
    "    - p (numpy.ndarray): 1D array of class probabilities (must sum to 1).\n",
    "\n",
    "    Returns:\n",
    "    - float: The cross-entropy value.\n",
    "    \"\"\"\n",
    "    ... # TODO\n",
    "    \n",
    "def compute_gini(p):\n",
    "    \"\"\"\n",
    "    Compute the Gini impurity of a probability distribution.\n",
    "\n",
    "    Parameters:\n",
    "    - p (numpy.ndarray): 1D array of class probabilities (must sum to 1).\n",
    "\n",
    "    Returns:\n",
    "    - float: The Gini impurity value.\n",
    "    \"\"\"\n",
    "    ... # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1789ac09-d5d4-4ad4-b3ce-3441dc5a1ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "def compute_cross_entropy(p):\n",
    "    \"\"\"\n",
    "    Compute the cross-entropy of a probability distribution.\n",
    "\n",
    "    Parameters:\n",
    "    - p (numpy.ndarray): 1D array of class probabilities (must sum to 1).\n",
    "\n",
    "    Returns:\n",
    "    - float: The cross-entropy value.\n",
    "    \"\"\"\n",
    "    return -np.sum(p * np.log2(p + 1e-15))  # Add small value to avoid log(0)\n",
    "\n",
    "def compute_gini(p):\n",
    "    \"\"\"\n",
    "    Compute the Gini impurity of a probability distribution.\n",
    "\n",
    "    Parameters:\n",
    "    - p (numpy.ndarray): 1D array of class probabilities (must sum to 1).\n",
    "\n",
    "    Returns:\n",
    "    - float: The Gini impurity value.\n",
    "    \"\"\"\n",
    "    return np.sum(p * (1 - p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d58dc7-2ded-4001-b870-f02b5b0cfd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "# These are some test cases to help you check if your implementation is correct. Each test case has an associated expected response.\n",
    "\n",
    "print(compute_cross_entropy(np.array([0.5, 0.5])))   # 1\n",
    "print(compute_cross_entropy(np.array([0, 1])))       # 0, if you got a NaN, it is being caused by a log(0)\n",
    "print(compute_gini(np.array([0.5, 0.5])))            # 0.5\n",
    "print(compute_gini(np.array([0, 1])))                # 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ad046-b4e9-42ae-b592-8594f125645c",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "We will also need a function that computes the class distribution given an array of class labels. For instance:\n",
    "\n",
    "1. [0, 0, 0, 1, 1, 1] - returns a class distribution [0.5, 0.5].\n",
    "2. [0, 1, 1, 1, 1, 1] - returns a class distribution [1/6, 5/6].\n",
    "\n",
    "Try to implement a generic function that works for any number of class labels of any data type such as natural numbers 0, 1, 2, ... and strings 'a', 'b', 'c', etc.\n",
    "\n",
    "Implement a function ``compute_class_distribution(labels)`` that returns a vector with the class distribution of the class labels in ``labels``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f65919-a21d-4b8d-98e6-8b1c915b5f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_distribution(labels):\n",
    "    \"\"\"\n",
    "    Compute the proportion of each class in the given labels.\n",
    "\n",
    "    Parameters:\n",
    "    - labels (numpy.ndarray): 1D array of class labels.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: 1D array of class proportions.\n",
    "    \"\"\"\n",
    "    ... # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a393e3e-b861-402f-841d-9468d5f9dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "def compute_class_distribution(labels):\n",
    "    \"\"\"\n",
    "    Compute the proportion of each class in the given labels.\n",
    "\n",
    "    Parameters:\n",
    "    - labels (numpy.ndarray): 1D array of class labels.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: 1D array of class proportions.\n",
    "    \"\"\"\n",
    "    total = len(labels)\n",
    "    return np.array([np.sum(labels == i) / total for i in np.unique(labels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a42902-518d-4157-a4aa-b6942dda7e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "# These are some test cases to help you check if your implementation is correct. Each test case has an associated expected response.\n",
    "\n",
    "print(compute_class_distribution(np.array([0, 0, 0, 1, 1, 1])))               # [0.5, 0.5]\n",
    "print(compute_class_distribution(np.array([0, 0, 0, 0, 0, 1])))               # [5/6, 1/6]\n",
    "print(compute_class_distribution(np.array(['a', 'a', 'b', 'b', 'c', 'c'])))   # [1/3, 1/3, 1/3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43018f0e-f37c-423e-9fd3-493a9b939f74",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Finally, let's compute the cut points for the Gini index and cross-entropy for a given numeric attribute $X$.\n",
    "\n",
    "The attribute will assume a series of values for different examples, for instance:\n",
    "\n",
    "$X$ = [6, 2, 8, 4, 6, 9, 1, 3, 5, 7, 9]\n",
    "\n",
    "We will sort the unique values of $X$:\n",
    "\n",
    "$X$ = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "And we will compute the mid-points between consecutive values:\n",
    "\n",
    "cut-points = [1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5]\n",
    "\n",
    "Implement a function ``compute_cut_points(X)`` that returns an array of cut-points given an input array of values for a feature ``X``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56928603-f7b9-4516-9d25-5215e2162fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cut_points(X):\n",
    "    \"\"\"\n",
    "    Compute the cut-points as mid-points for consecutive values of X.\n",
    "\n",
    "    Parameters:\n",
    "    - X (numpy.ndarray): 1D array of feature values.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: 1D array of cut-points.\n",
    "    \"\"\"\n",
    "    ... # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0c336d-b7b6-4150-acff-a1604cb82f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cut_points(X):\n",
    "    \"\"\"\n",
    "    Compute the cut-points as mid-points for consecutive values of X.\n",
    "\n",
    "    Parameters:\n",
    "    - X (numpy.ndarray): 1D array of feature values.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: 1D array of cut-points.\n",
    "    \"\"\"\n",
    "    sorted_X = np.sort(np.unique(X))\n",
    "    cutpoints = (sorted_X[:-1] + sorted_X[1:]) / 2\n",
    "    return cutpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d944364-a9fc-4ec5-86b6-bd02b2112d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "# These are some test cases to help you check if your implementation is correct. Each test case has an associated expected response.\n",
    "\n",
    "print(compute_cut_points(np.array([6, 2, 8, 4, 6, 9, 1, 3, 5, 7, 9])))      # [1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b10a186-2245-4ed0-8bcb-a3dccd2201e7",
   "metadata": {},
   "source": [
    "## Putting everything together\n",
    "\n",
    "Let's use the implemented functions to measure the Gini index and cross-entropy for the petal length attribute.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Implement a code that measures the Gini index and cross-entropy for all cut-points of the petal length attribute. We have done most of the code for you. You just need to fill in a few gaps with calls to your functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49c2961-ef80-4ddf-b1dc-fe386c4be051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract petal length and width features and the target variable\n",
    "petal_length = iris_df['petal length (cm)'].values\n",
    "petal_width = iris_df['petal width (cm)'].values\n",
    "target = iris_df['target'].values\n",
    "\n",
    "# Compute the cut-points for the petal_length attribute\n",
    "cutpoints = ...                                                # TODO\n",
    "\n",
    "# Store results for cross-entropy and Gini\n",
    "results = []\n",
    "\n",
    "# Iterate over each cutpoint\n",
    "for cut in cutpoints:\n",
    "    # Split the data into two groups based on the cutpoint\n",
    "    left_group = target[petal_length < cut]\n",
    "    right_group = target[petal_length >= cut]\n",
    "\n",
    "    # Compute class distributions for both groups\n",
    "    p_left = ...                                               # TODO\n",
    "    p_right = ...                                              # TODO\n",
    "\n",
    "    # Compute cross-entropy for both groups\n",
    "    ce_left = ...                                              # TODO\n",
    "    ce_right = ...                                             # TODO\n",
    "\n",
    "    # Compute Gini impurity for both groups\n",
    "    gini_left = ...                                            # TODO\n",
    "    gini_right = ...                                           # TODO\n",
    "    \n",
    "    # Weighted averages of cross-entropy and Gini impurity\n",
    "    total_size = len(left_group) + len(right_group)\n",
    "    weighted_ce = (len(left_group) / total_size) * ce_left + (len(right_group) / total_size) * ce_right\n",
    "    weighted_gini = (len(left_group) / total_size) * gini_left + (len(right_group) / total_size) * gini_right\n",
    "\n",
    "    # Store the cutpoint and corresponding metrics\n",
    "    results.append((cut, weighted_ce, weighted_gini))\n",
    "\n",
    "# Convert results to DataFrame for easier plotting\n",
    "metrics_df = pd.DataFrame(results, columns=['Cutpoint', 'Cross-Entropy', 'Gini'])\n",
    "\n",
    "# Identify the cutpoint with minimal cross-entropy\n",
    "min_ce_cutpoint = metrics_df.loc[metrics_df['Cross-Entropy'].idxmin(), 'Cutpoint']\n",
    "\n",
    "# Plot Cross-Entropy and Gini Impurity for each Cutpoint\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(metrics_df['Cutpoint'], metrics_df['Cross-Entropy'], marker='o', linestyle='-', label='Cross-Entropy', color='b')\n",
    "plt.plot(metrics_df['Cutpoint'], metrics_df['Gini'], marker='s', linestyle='--', label='Gini Impurity', color='g')\n",
    "plt.axvline(x=min_ce_cutpoint, color='r', linestyle='--', label=f'Min CE Cutpoint: {min_ce_cutpoint:.2f}')\n",
    "plt.title('Cross-Entropy and Gini vs. Cutpoint for Petal Length')\n",
    "plt.xlabel('Cutpoint (Petal Length)')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of Petal Width vs. Petal Length with the minimal CE cutpoint\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(petal_length, petal_width, c=target, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.axvline(x=min_ce_cutpoint, color='r', linestyle='--', label=f'Min CE Cutpoint: {min_ce_cutpoint:.2f}')\n",
    "plt.title('Petal Width vs. Petal Length with Minimal Cross-Entropy Cutpoint')\n",
    "plt.xlabel('Petal Length (cm)')\n",
    "plt.ylabel('Petal Width (cm)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887a8a84-8419-4370-af8d-c340dfabf2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "# Extract petal length and width features and the target variable\n",
    "petal_length = iris_df['petal length (cm)'].values\n",
    "petal_width = iris_df['petal width (cm)'].values\n",
    "target = iris_df['target'].values\n",
    "\n",
    "# Compute the cut-points for the petal_length attribute\n",
    "cutpoints = compute_cut_points(petal_length)\n",
    "\n",
    "# Store results for cross-entropy and Gini\n",
    "results = []\n",
    "\n",
    "# Iterate over each cutpoint\n",
    "for cut in cutpoints:\n",
    "    # Split the data into two groups based on the cutpoint\n",
    "    left_group = target[petal_length < cut]\n",
    "    right_group = target[petal_length >= cut]\n",
    "\n",
    "    # Compute class distributions for both groups\n",
    "    p_left = compute_class_distribution(left_group)\n",
    "    p_right = compute_class_distribution(right_group)\n",
    "\n",
    "    # Compute cross-entropy for both groups\n",
    "    ce_left = compute_cross_entropy(p_left)\n",
    "    ce_right = compute_cross_entropy(p_right)\n",
    "\n",
    "    # Compute Gini impurity for both groups\n",
    "    gini_left = compute_gini(p_left)\n",
    "    gini_right = compute_gini(p_right)\n",
    "    \n",
    "    # Weighted averages of cross-entropy and Gini impurity\n",
    "    total_size = len(left_group) + len(right_group)\n",
    "    weighted_ce = (len(left_group) / total_size) * ce_left + (len(right_group) / total_size) * ce_right\n",
    "    weighted_gini = (len(left_group) / total_size) * gini_left + (len(right_group) / total_size) * gini_right\n",
    "\n",
    "    # Store the cutpoint and corresponding metrics\n",
    "    results.append((cut, weighted_ce, weighted_gini))\n",
    "\n",
    "# Convert results to DataFrame for easier plotting\n",
    "metrics_df = pd.DataFrame(results, columns=['Cutpoint', 'Cross-Entropy', 'Gini'])\n",
    "\n",
    "# Identify the cutpoint with minimal cross-entropy\n",
    "min_ce_cutpoint = metrics_df.loc[metrics_df['Cross-Entropy'].idxmin(), 'Cutpoint']\n",
    "\n",
    "# Plot Cross-Entropy and Gini Impurity for each Cutpoint\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(metrics_df['Cutpoint'], metrics_df['Cross-Entropy'], marker='o', linestyle='-', label='Cross-Entropy', color='b')\n",
    "plt.plot(metrics_df['Cutpoint'], metrics_df['Gini'], marker='s', linestyle='--', label='Gini Impurity', color='g')\n",
    "plt.axvline(x=min_ce_cutpoint, color='r', linestyle='--', label=f'Min CE Cutpoint: {min_ce_cutpoint:.2f}')\n",
    "plt.title('Cross-Entropy and Gini vs. Cutpoint for Petal Length')\n",
    "plt.xlabel('Cutpoint (Petal Length)')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of Petal Width vs. Petal Length with the minimal CE cutpoint\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(petal_length, petal_width, c=target, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.axvline(x=min_ce_cutpoint, color='r', linestyle='--', label=f'Min CE Cutpoint: {min_ce_cutpoint:.2f}')\n",
    "plt.title('Petal Width vs. Petal Length with Minimal Cross-Entropy Cutpoint')\n",
    "plt.xlabel('Petal Length (cm)')\n",
    "plt.ylabel('Petal Width (cm)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1569dfd-84c2-49f4-b32e-a7879fdd048e",
   "metadata": {},
   "source": [
    "The plots show that the Gini index and cross-entropy are strongly correlated. Both indicate that the best cut-point is around 2.45, which gives us a good separation margin between the classes setosa and versicolor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a51ff57-2980-4f7e-a1a9-85d96f5e5efd",
   "metadata": {},
   "source": [
    "## Training our first decision tree classifier\n",
    "\n",
    "Let's train our first decision tree classifier using the iris dataset. For now, we will only focus on training and model interpretability. In the next section, we will also evaluate and compare the decision tree models with their ensemble versions.\n",
    "\n",
    "The next cell will split the Pandas dataframe into input features and class attribute and use the Scikit-learn library to create and a graphical representation of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445b8e89-9af1-46b4-aa12-5cb3da9e554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the DataFrame into attributes (features) and class labels\n",
    "X = iris_df.iloc[:, :-2]  # All columns except the last one (features)\n",
    "y = iris_df.iloc[:, -1]   # The last column (class labels)\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(15, 10))\n",
    "tree.plot_tree(clf, feature_names=list(X.columns), class_names=list(iris.target_names), filled=True)\n",
    "plt.title(\"Decision Tree Trained on Iris Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26ffe24-8fef-417b-b51e-c07d8ed7c7a7",
   "metadata": {},
   "source": [
    "### Description of the tree model\n",
    "\n",
    "We can see how the decision tree splits the cases into regions. The root split is petal length <= 2.45, as we expected, creating a leaf note for the setosa class with all 50 cases of that plant. \n",
    "\n",
    "As we go down in the tree, we can see that some cases of Versicolor and Virginia are simple to separate, creating large leaf nodes with 47 (Versicolor) and 43 cases (Virginia) out of 50 cases of each class. However, other cases are difficult to separate, creating leaf nodes with one, two, or three cases. These nodes are prone to overfitting.\n",
    "\n",
    "The next cell visualises all cut points and regions created for this decision tree. The cell is ready to run. You can change the attributes in the scatterplot to see how the decision tree splits across the other attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45f553c-0ad6-41bc-9d4b-8bf54a078446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "def plot_decision_tree_boundary(df, attribute1, attribute2):\n",
    "    \"\"\"\n",
    "    Plots the decision boundaries for a Decision Tree Classifier using two selected attributes.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame containing the data.\n",
    "    - attribute1 (str): The name of the first attribute to use for plotting.\n",
    "    - attribute2 (str): The name of the second attribute to use for plotting.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the selected attributes and class labels\n",
    "    X = df[[attribute1, attribute2]].values\n",
    "    y = df['target'].values\n",
    "\n",
    "    # Initialize and train the Decision Tree Classifier\n",
    "    clf = DecisionTreeClassifier(random_state=42)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Define the mesh grid for decision boundaries\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                         np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "    # Predict the class for each point in the mesh grid\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot the decision boundaries\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "    plt.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.6)\n",
    "\n",
    "    # Plot the training points\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', s=50, cmap='viridis')\n",
    "    plt.xlabel(attribute1)\n",
    "    plt.ylabel(attribute2)\n",
    "    plt.title(f'Decision Tree Decision Boundaries ({attribute1} vs. {attribute2})')\n",
    "    plt.colorbar(scatter, label='Target Class')\n",
    "\n",
    "    # Plot decision cutpoints (vertical or horizontal lines)\n",
    "    n_nodes = clf.tree_.node_count\n",
    "    feature = clf.tree_.feature\n",
    "    threshold = clf.tree_.threshold\n",
    "\n",
    "    for i in range(n_nodes):\n",
    "        if feature[i] == 0:  # Attribute 1 (x-axis)\n",
    "            plt.axvline(x=threshold[i], color='r', linestyle='--', label=f'Cutpoint: {threshold[i]:.2f}')\n",
    "        elif feature[i] == 1:  # Attribute 2 (y-axis)\n",
    "            plt.axhline(y=threshold[i], color='b', linestyle='--', label=f'Cutpoint: {threshold[i]:.2f}')\n",
    "\n",
    "    # Avoid duplicate legend entries\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys(), loc='best')\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage: Plot using 'petal length (cm)' and 'petal width (cm)'\n",
    "plot_decision_tree_boundary(iris_df, 'petal length (cm)', 'petal width (cm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36f4875-eb1a-45c0-b489-d48ba7125c6e",
   "metadata": {},
   "source": [
    "Let's use a larger and more complicated dataset that will allow us to train and compare trees and forest models.\n",
    "\n",
    "## The wine quality dataset.\n",
    "\n",
    "The Wine Quality dataset contains the chemical properties of red and white wines and their quality ratings, scored on a scale from 0 to 10. Each row represents a specific wine sample, with features including fixed acidity, volatile acidity, citric acid, chlorides, and density.\n",
    "\n",
    "We will use this dataset for regression (predicting exact quality scores) and classification (predicting if a wine is bad, for scores of 0 to 5, or good, for scores of 6 to 10).\n",
    "\n",
    "Let's start by loading the dataset from the UCI data repository, separating the features from the target attribute and creating a binary class label where 0 corresponds to scores below and equal to 5 and 1 to scores greater than 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bf6d17-64af-46e6-b1f5-761f57236cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Wine Quality dataset\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "wine_df = pd.read_csv(url, delimiter=';')\n",
    "\n",
    "# Separate features and target\n",
    "X = wine_df.iloc[:, :-1]  # All columns except the last (features)\n",
    "y = wine_df.iloc[:, -1]   # Last column (quality score)\n",
    "\n",
    "# Convert wine quality into binary classes for classification (>=6 as good, <6 as bad)\n",
    "y_class = (y >= 6).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052e7824-6fc0-47c5-b740-0814f3c3e3e7",
   "metadata": {},
   "source": [
    "The wine dataset does not have a standard split of training and test partitions. So, we will create one with 80% for training and 20% for testing. We will call the Scikit-learn function `train_test_split` twice, as we have a classification and a regression target attribute.\n",
    "\n",
    "As you will see, we will often use ``random_state=42``. This statement sets the seed of the random number generator. In most cases, we use this statement to enforce that different executions will provide the same results, even when the algorithms involve stochastic decisions. \n",
    "\n",
    "In the next cell, ``random_state=42`` plays an important role, as it enforces that the two executions of ``train_test_split`` will provide the same data partitioning. Notice that the first call splits input features and class labels, but the second only splits the regression targets and uses the input split of the first call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f80832-b2ad-417b-a5f2-1029dc2d4b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train_class, y_test_class = train_test_split(X, y_class, test_size=0.2, random_state=42)\n",
    "_, _, y_train_reg, y_test_reg = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bb250a-7dd9-4520-9305-619ed28e1527",
   "metadata": {},
   "source": [
    "## Training tree-based classifiers and regressors\n",
    "\n",
    "Great, we can now train our models. We will show you how to do it for the decision tree classifier and let you do the same for the bagging and random forest classifiers. We will use the default hyperparameters for now and see how to search for them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63b3db-7c4a-46db-91ae-2904a4c1f8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the decision tree model for classification\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(X_train, y_train_class)\n",
    "y_pred_class = dt_classifier.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_class, y_pred_class))\n",
    "\n",
    "# Generate confusion matrix and plot it\n",
    "conf_matrix = confusion_matrix(y_test_class, y_pred_class)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "plt.title('Confusion Matrix for Decision Tree Classifier')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Initialize decision tree model for regression\n",
    "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
    "dt_regressor.fit(X_train, y_train_reg)\n",
    "y_pred_reg = dt_regressor.predict(X_test)\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test_reg, y_pred_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caa4877-425d-477c-934f-1d2b297017b2",
   "metadata": {},
   "source": [
    "Great, you should have got an accuracy of around 72% abd a MSE of 0.61. Remember that MSE is an error measure, so lower values are better. Accuracy is a measure of correctness, so higher values are better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa98ae-3c25-4c9b-97bf-a47a636697d1",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "It is your turn. We will do the same for the bagging of decision trees and the random forest model.\n",
    "\n",
    "We start by bagging decision trees. We have created an initial code; you just need to fill in the missing parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156fccda-c24c-4678-b8a5-04c7e474cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the bagging of trees model for classification\n",
    "bagging_classifier = BaggingClassifier(estimator=DecisionTreeClassifier(), random_state=42)\n",
    "... # TODO: fit the model to training data\n",
    "... # TODO: predict the class of test instances\n",
    "... # TODO: measure and print the accuracy of the model on test data\n",
    "\n",
    "# Generate confusion matrix and plot it\n",
    "conf_matrix = confusion_matrix(y_test_class, y_pred_class)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "plt.title('Confusion Matrix for Decision Tree Classifier')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Initialize bagging of tree model for regression\n",
    "bagging_regressor = BaggingRegressor(estimator=DecisionTreeRegressor(), random_state=42)\n",
    "... # TODO: fit the model to training data\n",
    "... # TODO: predict the target of the test instances\n",
    "... # TODO: measure and print the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d42e654-27b6-43de-b154-db6d28e110f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "# Initialize the bagging of trees model for classification\n",
    "bagging_classifier = BaggingClassifier(estimator=DecisionTreeClassifier(), random_state=42)\n",
    "bagging_classifier.fit(X_train, y_train_class)\n",
    "y_pred_class = bagging_classifier.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_class, y_pred_class))\n",
    "\n",
    "# Generate confusion matrix and plot it\n",
    "conf_matrix = confusion_matrix(y_test_class, y_pred_class)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "plt.title('Confusion Matrix for Decision Tree Classifier')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Initialize bagging of tree model for regression\n",
    "bagging_regressor = BaggingRegressor(estimator=DecisionTreeRegressor(), random_state=42)\n",
    "bagging_regressor.fit(X_train, y_train_reg)\n",
    "y_pred_reg = bagging_regressor.predict(X_test)\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test_reg, y_pred_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e6bfd-3d41-456f-ab5b-cbae49c98a4d",
   "metadata": {},
   "source": [
    "Here are a couple of observations about the bagging of decision trees\n",
    "\n",
    "1. The technique significantly improves accuracy and reduced MSE. The reduction in MSE is notable.\n",
    "2. Bagging is a technique that can be applied to different classifiers. Therefore, we need to specify a (base) estimator. You can use any base estimator besides decision trees, including neural nets.\n",
    "\n",
    "Next, we will create a similar code for the Random Forest classifier. Although it is very similar to bagging of tree classifiers, the random forest only uses decision trees as base estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0beea99-1e01-47c6-8132-e312a7b23cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest model for classification\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "... # TODO: fit the model to training data\n",
    "... # TODO: predict the class of test instances\n",
    "... # TODO: measure and print the accuracy of the model on test data\n",
    "\n",
    "# Generate confusion matrix and plot it\n",
    "conf_matrix = confusion_matrix(y_test_class, y_pred_class)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "plt.title('Confusion Matrix for Decision Tree Classifier')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Initialize bagging of tree model for regression\n",
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "... # TODO: fit the model to training data\n",
    "... # TODO: predict the target of the test instances\n",
    "... # TODO: measure and print the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d091627f-5781-459d-add9-537438aa790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "# Initialize the bagging of trees model for classification\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_classifier.fit(X_train, y_train_class)\n",
    "y_pred_class = rf_classifier.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_class, y_pred_class))\n",
    "\n",
    "# Generate confusion matrix and plot it\n",
    "conf_matrix = confusion_matrix(y_test_class, y_pred_class)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "plt.title('Confusion Matrix for Decision Tree Classifier')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Initialize bagging of tree model for regression\n",
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "rf_regressor.fit(X_train, y_train_reg)\n",
    "y_pred_reg = rf_regressor.predict(X_test)\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test_reg, y_pred_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc99610f-e681-419b-804b-cd761a0c58c2",
   "metadata": {},
   "source": [
    "This time, we see minor improvements in accuracy and MSE compared to the bagging model.\n",
    "\n",
    "## Searching for hyperparameters\n",
    "\n",
    "Let's see if we can improve the performance of the classifiers and regressors for the Wine dataset. We will start with the decision tree model and show you how it is done, then let you code a similar search for the ensemble models.\n",
    "\n",
    "We will use the Scikit-learn class GridSearchCV to implement the search procedure. GridSearchCV exhaustively searches through a predefined grid of hyperparameter combinations. The search performs k-fold cross-validation for each hyperparameter combination. In these experiments, we will perform 5-fold cross-validation.\n",
    "\n",
    "For the tree models, we have the following recommendations:\n",
    "\n",
    "1. ``max_depth``: Sets the maximum depth of the tree, controlling how deep the tree can grow. Deeper trees may overfit, while shallower trees generalize better.\n",
    "2. ``min_samples_split``: The minimum number of samples required to split an internal node. Higher values prevent over-splitting and control overfitting by enforcing larger splits.\n",
    "3. ``min_samples_leaf``: The minimum number of samples a leaf node must contain. Larger values create broader leaves.\n",
    "3. ``ccp_alpha``: This is the cost complexity pruning parameter. A higher value increases pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4937ca-6a19-456a-8c1b-7b7621b95c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'ccp_alpha': [0.0, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Initialize the Decision Tree model\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Perform Grid Search for Classification\n",
    "grid_search_clf = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_clf.fit(X_train, y_train_class)\n",
    "\n",
    "print(\"Best Classifier Parameters:\", grid_search_clf.best_params_)\n",
    "best_clf = grid_search_clf.best_estimator_\n",
    "y_pred_class = best_clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_class, y_pred_class))\n",
    "\n",
    "# Generate confusion matrix and plot it\n",
    "conf_matrix = confusion_matrix(y_test_class, y_pred_class)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "plt.title('Confusion Matrix for Decision Tree Classifier')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Initialize the Regression Tree model\n",
    "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Perform Grid Search for Regression\n",
    "grid_search_reg = GridSearchCV(estimator=dt_regressor, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_reg.fit(X_train, y_train_reg)\n",
    "\n",
    "print(\"\\nBest Regressor Parameters:\", grid_search_reg.best_params_)\n",
    "best_reg = grid_search_reg.best_estimator_\n",
    "y_pred_reg = best_reg.predict(X_test)\n",
    "print(\"\\nRegression Performance:\")\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test_reg, y_pred_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb2349f-3e03-448a-a61e-beef6995b6e7",
   "metadata": {},
   "source": [
    "We can see some improvement in MSE, but it cannot achieve the level of the ensemble methods. Our search did not improve the classification performance, though.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Now it is your turn. Let's see if we can improve the performance of bagging decision trees with hyperparameter optimisation. Regarding the hyperparameters to search, we will search the same ones used for the trees and one specific of the ensemble: ``n_estimators``. This hyperparameter controls the number of trees created.\n",
    "\n",
    "We have made most of the code and will let you complete a few gaps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17ad380-dccc-4b3e-954a-d8acba513bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for bagging models\n",
    "param_grid = {\n",
    "    'estimator__max_depth': [3, 5, 10],\n",
    "    'estimator__min_samples_split': [2, 5, 10],\n",
    "    'estimator__min_samples_leaf': [1, 2, 4],\n",
    "    'estimator__ccp_alpha': [0.0, 0.01, 0.1],\n",
    "    'n_estimators': [10, 50, 100]\n",
    "}\n",
    "\n",
    "# Initialize the Bagging classification model with Decision Trees as base estimators\n",
    "bagging_clf = ...                                                  # TODO: initialise the decision tree bagging classifier\n",
    "\n",
    "# Perform Grid Search for Bagging Classifier\n",
    "grid_search_clf = ...                                              # TODO: create and initialise the grid search object\n",
    "grid_search_clf.fit(X_train, y_train_class)\n",
    "\n",
    "print(\"Best Classifier Parameters:\", grid_search_clf.best_params_)\n",
    "best_clf = grid_search_clf.best_estimator_\n",
    "y_pred_class = best_clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_class, y_pred_class))\n",
    "\n",
    "# Generate confusion matrix and plot it\n",
    "conf_matrix = confusion_matrix(y_test_class, y_pred_class)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "plt.title('Confusion Matrix for Decision Tree Classifier')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Initialize the Bagging regression model with Decision Trees as base estimators\n",
    "bagging_reg = ...                                                # TODO: initialise decision tree bagging regressor\n",
    "\n",
    "# Perform Grid Search for Bagging Regressor\n",
    "grid_search_reg = ...                                            # TODO: create and initialise the grid search object\n",
    "grid_search_reg.fit(X_train, y_train_reg)\n",
    "\n",
    "print(\"\\nBest Regressor Parameters:\", grid_search_reg.best_params_)\n",
    "best_reg = grid_search_reg.best_estimator_\n",
    "y_pred_reg = best_reg.predict(X_test)\n",
    "print(\"\\nRegression Performance:\")\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test_reg, y_pred_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55b2b43-307c-44bc-a155-275975989954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "# Define parameter grid for bagging models\n",
    "param_grid = {\n",
    "    'estimator__max_depth': [3, 5, 10],\n",
    "    'estimator__min_samples_split': [2, 5, 10],\n",
    "    'estimator__min_samples_leaf': [1, 2, 4],\n",
    "    'estimator__ccp_alpha': [0.0, 0.01, 0.1],\n",
    "    'n_estimators': [10, 50, 100]\n",
    "}\n",
    "\n",
    "# Initialize the Bagging classification model with Decision Trees as base estimators\n",
    "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(random_state=42), random_state=42)\n",
    "\n",
    "# Perform Grid Search for Bagging Classifier\n",
    "grid_search_clf = GridSearchCV(estimator=bagging_clf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_clf.fit(X_train, y_train_class)\n",
    "\n",
    "print(\"Best Classifier Parameters:\", grid_search_clf.best_params_)\n",
    "best_clf = grid_search_clf.best_estimator_\n",
    "y_pred_class = best_clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_class, y_pred_class))\n",
    "\n",
    "# Generate confusion matrix and plot it\n",
    "conf_matrix = confusion_matrix(y_test_class, y_pred_class)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "plt.title('Confusion Matrix for Decision Tree Classifier')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Initialize the Bagging regression model with Decision Trees as base estimators\n",
    "bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(random_state=42), random_state=42)\n",
    "\n",
    "# Perform Grid Search for Bagging Regressor\n",
    "grid_search_reg = GridSearchCV(estimator=bagging_reg, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_reg.fit(X_train, y_train_reg)\n",
    "\n",
    "print(\"\\nBest Regressor Parameters:\", grid_search_reg.best_params_)\n",
    "best_reg = grid_search_reg.best_estimator_\n",
    "y_pred_reg = best_reg.predict(X_test)\n",
    "print(\"\\nRegression Performance:\")\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test_reg, y_pred_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcb5a94-0fa0-4a50-b704-59e19ddbe774",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "As our final task, we will implement a hyperparameter search for the Random Forest classifier and regressor. Again, we use the same hyperparameter grid for bagging trees.\n",
    "\n",
    "We have implemented most of the code, leaving just a few parts for you to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f754571-b6dc-4e25-a21a-e085b91fdace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of trees in the forest\n",
    "    'max_depth': [3, 5, 10],         # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5, 10], # Minimum samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4],   # Minimum samples required to form a leaf\n",
    "    'ccp_alpha': [0.0, 0.01, 0.1]    # Complexity parameter for pruning\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest model for classification\n",
    "rf_classifier = ...                                                  # TODO: initialise the Random Forest classifier\n",
    "\n",
    "# Perform Grid Search for Random Forest Classifier\n",
    "grid_search_clf = ...                                                # TODO: create and initialise the grid search object\n",
    "grid_search_clf.fit(X_train, y_train_class)\n",
    "\n",
    "print(\"Best Classifier Parameters:\", grid_search_clf.best_params_)\n",
    "best_clf = grid_search_clf.best_estimator_\n",
    "y_pred_class = best_clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_class, y_pred_class))\n",
    "\n",
    "# Generate confusion matrix and plot it\n",
    "conf_matrix = confusion_matrix(y_test_class, y_pred_class)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "plt.title('Confusion Matrix for Decision Tree Classifier')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Initialize the Random Forest model for regression\n",
    "rf_regressor = ...                                                   # TODO: initialise the Random Forest regressor\n",
    "\n",
    "# Perform Grid Search for Random Forest Regressor\n",
    "grid_search_reg = ...                                                # TODO: create and initialise the grid search object\n",
    "grid_search_reg.fit(X_train, y_train_reg)\n",
    "\n",
    "print(\"\\nBest Regressor Parameters:\", grid_search_reg.best_params_)\n",
    "best_reg = grid_search_reg.best_estimator_\n",
    "y_pred_reg = best_reg.predict(X_test)\n",
    "print(\"\\nRegression Performance:\")\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test_reg, y_pred_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53869f88-db21-4141-bdf3-67967b90f492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "# Define parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of trees in the forest\n",
    "    'max_depth': [3, 5, 10],         # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5, 10], # Minimum samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4],   # Minimum samples required to form a leaf\n",
    "    'ccp_alpha': [0.0, 0.01, 0.1]    # Complexity parameter for pruning\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest model for classification\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform Grid Search for Random Forest Classifier\n",
    "grid_search_clf = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_clf.fit(X_train, y_train_class)\n",
    "\n",
    "print(\"Best Classifier Parameters:\", grid_search_clf.best_params_)\n",
    "best_clf = grid_search_clf.best_estimator_\n",
    "y_pred_class = best_clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_class, y_pred_class))\n",
    "\n",
    "# Generate confusion matrix and plot it\n",
    "conf_matrix = confusion_matrix(y_test_class, y_pred_class)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "plt.title('Confusion Matrix for Decision Tree Classifier')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Initialize the Random Forest model for regression\n",
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Perform Grid Search for Random Forest Regressor\n",
    "grid_search_reg = GridSearchCV(estimator=rf_regressor, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_reg.fit(X_train, y_train_reg)\n",
    "\n",
    "print(\"\\nBest Regressor Parameters:\", grid_search_reg.best_params_)\n",
    "best_reg = grid_search_reg.best_estimator_\n",
    "y_pred_reg = best_reg.predict(X_test)\n",
    "print(\"\\nRegression Performance:\")\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test_reg, y_pred_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d680eaa1-35b4-4328-ad82-13668e967fa8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Today, we have learned how to implement and evaluate decision tree classifiers and regressors, including ensemble models. These models are excellent choices, particularly if the data is tabular.\n",
    "\n",
    "Unfortunately, the Scikit-learn implementation of tree models is pretty basic. For instance, these models cannot handle qualitative (discrete) attributes. However, it is natural for tree models to create splits such as $X_1$ == 'red' and $X_1$ != 'green'.\n",
    "\n",
    "Two more advanced implementations of ensemble decision trees are the highly popular [XGBoost](https://xgboost.readthedocs.io/) and the highly efficient [LightGBM](https://lightgbm.readthedocs.io/). These are variations of the [Random Forest classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) that use [boosting](https://en.wikipedia.org/wiki/Boosting_(machine_learning)) to implement the ensembles.\n",
    "\n",
    "We hope that you have enjoyed the Machine Learning tutorials!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
